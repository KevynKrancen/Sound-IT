{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Holistic and Drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the image\n",
    "file_path = \"/Users/kevynkrancenblum/Desktop/Data Science/Final Project/streamlittest/test.png\"\n",
    "image = cv2.imread(file_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Process the image with MediaPipe Holistic\n",
    "with mp_holistic.Holistic(static_image_mode=True) as holistic:\n",
    "    results = holistic.process(image_rgb)\n",
    "\n",
    "# Draw keypoints on the image\n",
    "image_with_keypoints = image.copy()\n",
    "\n",
    "# Draw pose landmarks\n",
    "if results.pose_landmarks:\n",
    "    mp_drawing.draw_landmarks(image_with_keypoints, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "# Draw face landmarks\n",
    "if results.face_landmarks:\n",
    "    mp_drawing.draw_landmarks(image_with_keypoints, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "\n",
    "# Draw left hand landmarks\n",
    "if results.left_hand_landmarks:\n",
    "    mp_drawing.draw_landmarks(image_with_keypoints, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "# Draw right hand landmarks\n",
    "if results.right_hand_landmarks:\n",
    "    mp_drawing.draw_landmarks(image_with_keypoints, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "# Save the image with keypoints\n",
    "output_file = \"/Users/kevynkrancenblum/Desktop/Data Science/Final Project/streamlittest/test_with_keypoints.png\"\n",
    "cv2.imwrite(output_file, image_with_keypoints)\n",
    "\n",
    "# Display the image with keypoints\n",
    "cv2.imshow('Image with Keypoints', image_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def ColorRecognition(path):\n",
    "    color = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    while True:\n",
    "        # Process Key (ESC: end)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        # Camera capture\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image = cv2.flip(image, 1)\n",
    "        average_color_row = np.average(image, axis=0)\n",
    "        average_color = np.average(average_color_row, axis=0)\n",
    "        d_img = np.ones((312, 312, 3), dtype=np.uint8)\n",
    "        d_img[:, :] = average_color\n",
    "        image.flags.writeable = False\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Classify emotions based on color\n",
    "        b, g, r = average_color.astype(int)\n",
    "        sentiment = None\n",
    "        if r >= 128 and g <= 160 and b <= 147:  # Red\n",
    "            color.append('Angry')\n",
    "            sentiment = 'Angry'\n",
    "        elif r <= 173 and g >= 100 and b <= 170:  # Green\n",
    "            color.append('Happy')\n",
    "            sentiment = 'Happy'\n",
    "        elif r <= 240 and g <= 248 and b >= 112:  # Blue\n",
    "            color.append('Sad')\n",
    "            sentiment = 'Sad'\n",
    "        elif r > 240 or g > 248 or b > 170:  # White\n",
    "            color.append('Neutral')\n",
    "            sentiment = 'Neutral'\n",
    "\n",
    "        # Display color box and sentiment\n",
    "# Display color box and sentiment\n",
    "        if sentiment:\n",
    "            cv2.rectangle(image, (10, 0), (image.shape[1], 90), (int(b), int(g), int(r)), -1)\n",
    "            cv2.putText(image, sentiment, (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow('Color Detection', image)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    def most_frequent(color):   \n",
    "        return (color.count('Angry'), color.count('Happy'), color.count('Sad'), color.count('Neutral'))\n",
    "\n",
    "    def pourcentageofeachfromlist(color):\n",
    "        return (color.count('Angry'), color.count('Happy'), color.count('Sad'), color.count('Neutral'))\n",
    "\n",
    "    return most_frequent(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 131, 0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ColorRecognition(\"/Users/kevynkrancenblum/Desktop/Data Science/Final Project/Facial_emotion_recognition/ModelVideos/happytest2.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
