<h1 align="center" style="font-size: 48px;">Sound-IT</h1>
Sound-IT is a research project aimed at using artificial intelligence to recognize emotions in videos and create music that fits those emotions. We have created four models to determine the emotional tone of a video: facial expression detection, background hue recognition, body emotion recognition, and lip reading. After that, music is generated that matches the detected emotion.

The proposed method offers a potential solution to the problem of high costs associated with composing and recording original music for films.

<h1 align="center" style="font-size: 48px;">How it works</h1>
Our model is capable of recognizing a scene's dominant emotion by analyzing factors such as body language, facial expressions, lip reading, and background color. Using this emotion, the model generates a new piece of music ideally suited to the scene.

<img width="331" alt="Screenshot 2023-03-30 at 11 32 57" src="https://user-images.githubusercontent.com/102467763/228778334-103d1250-7fdc-4f0b-be9a-6146af27f999.png"> <img width="331" alt="Screenshot 2023-03-30 at 11 33 13" src="https://user-images.githubusercontent.com/102467763/228778357-ea8cc5dc-8087-42be-bd3b-9f78d8307986.png">

Examples
Here are some examples of the emotions detected by our model and the corresponding music generated:

Example 1

Example 2

Getting Started
To get started with Sound-IT, you can clone our repository and follow the instructions in the README file.

Contributing
We welcome contributions from the community. If you have any suggestions or would like to contribute, please open an issue or pull request on our GitHub repository.

License
Sound-IT is licensed under the MIT License. See the LICENSE file for details.